{"nbformat_minor": 2, "cells": [{"source": "# VIII - Parallel and Distributed Execution\nIn this notebook, we will execute training across multiple nodes (or in parallel across a single node over multiple GPUs). We will train an image classification model with Resnet20 on the CIFAR-10 data set across multiple nodes in this notebook.\n\nAzure Batch and Batch Shipyard have the ability to perform \"gang scheduling\" or scheduling multiple nodes for a single task. This is most commonly used for Message Passing Interface (MPI) jobs.\n\n* [Setup](#section1)\n* [Configure and Submit MPI Job and Submit](#section2)\n* [Delete Multi-Instance Job](#section3)", "cell_type": "markdown", "metadata": {}}, {"source": "<a id='section1'><\/a>", "cell_type": "markdown", "metadata": {}}, {"source": "## Setup", "cell_type": "markdown", "metadata": {}}, {"source": "Create a simple alias for Batch Shipyard", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "%alias shipyard SHIPYARD_CONFIGDIR=config python $HOME/batch-shipyard/shipyard.py %l", "outputs": [], "metadata": {"collapsed": true}}, {"source": "Check that everything is working", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "shipyard", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Let's first delete the pool used in the non-advanced notebooks and wait for it to be removed so we can free up our core quota. We need to create a new pool with different settings and Docker image.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "shipyard pool del -y --wait", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Read in the account information we saved earlier", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "import json\nimport os\n\ndef read_json(filename):\n    with open(filename, 'r') as infile:\n        return json.load(infile)\n    \ndef write_json_to_file(json_dict, filename):\n    \"\"\" Simple function to write JSON dictionaries to files\n    \"\"\"\n    with open(filename, 'w') as outfile:\n        json.dump(json_dict, outfile)\n\naccount_info = read_json('account_information.json')\n\nstorage_account_key = account_info['storage_account_key']\nstorage_account_name = account_info['storage_account_name']", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Create the `resource_files` to randomly download train and test data for CNTK", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "import random\n\nIMAGE_NAME = 'alfpark/cntk:2.0.rc2-gpu-1bit-sgd-python3.5-cuda8.0-cudnn5.1'\n\nCNTK_TRAIN_DATA_FILE = 'Train_cntk_text.txt'\nCNTK_TEST_DATA_FILE = 'Test_cntk_text.txt'\nCNTK_DATA_BATCHES_FILE = 'cifar-10-batches-py.tar.gz'\nURL_FMT = 'https://{}.blob.core.windows.net/{}/{}'\n\ndef select_random_data_storage_container():\n    \"\"\"Randomly select a storage account and container for CNTK train/test data.\n    This is specific for the workshop to help distribute attendee load. This\n    function will only work on Python2\"\"\"\n    ss = random.randint(0, 4)\n    cs = random.randint(0, 4)\n    sa = '{}{}bigai'.format(ss, chr(ord('z') - ss))\n    cont = '{}{}{}'.format(cs, chr(ord('i') - cs * 2), chr(ord('j') - cs * 2))\n    return sa, cont\n\ndef create_resource_file_list():\n    sa, cont = select_random_data_storage_container()\n    ret = [{\n        'file_path': CNTK_TRAIN_DATA_FILE,\n        'blob_source': URL_FMT.format(sa, cont, CNTK_TRAIN_DATA_FILE)\n    }]\n    sa, cont = select_random_data_storage_container()\n    ret.append({\n        'file_path': CNTK_TEST_DATA_FILE,\n        'blob_source': URL_FMT.format(sa, cont, CNTK_TEST_DATA_FILE)\n    })\n    sa, cont = select_random_data_storage_container()\n    ret.append({\n        'file_path': CNTK_DATA_BATCHES_FILE,\n        'blob_source': URL_FMT.format(sa, cont, CNTK_DATA_BATCHES_FILE)\n    })\n    return ret", "outputs": [], "metadata": {"collapsed": true}}, {"source": "Create data set conversion scripts to be uploaded. On real production runs, we would already have this data pre-converted instead of converting at the time of node startup.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "%%writefile convert_cifar10.py\nfrom __future__ import print_function\nimport cifar_utils as ut\n\nprint ('Converting train data to png images...')\nut.saveTrainImages(r'./Train_cntk_text.txt', 'train')\nprint ('Done.')\nprint ('Converting test data to png images...')\nut.saveTestImages(r'./Test_cntk_text.txt', 'test')\nprint ('Done.')", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "%%writefile convert_cifar10.sh\n#!/usr/bin/env bash\n\nset -e\nset -o pipefail", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "with open('convert_cifar10.sh', 'a') as fd:\n    fd.write('\\n\\nIMAGE_NAME=\"{}\"\\n'.format(IMAGE_NAME))", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "%%writefile -a convert_cifar10.sh\nCIFAR_DATA=$AZ_BATCH_NODE_SHARED_DIR/cifar10_data\nCIFAR_BATCHES=cifar-10-batches-py.tar.gz\n\nmv $AZ_BATCH_TASK_WORKING_DIR/*_cntk_text.txt $AZ_BATCH_TASK_WORKING_DIR/$CIFAR_BATCHES $CIFAR_DATA\necho \"Converting CNTK train/test data, this will take some time...\"\npushd $CIFAR_DATA\ntar zxvpf $CIFAR_BATCHES\nrm $CIFAR_BATCHES\nchmod 755 run_cifar10.sh\nmv run_cifar10.sh $AZ_BATCH_NODE_SHARED_DIR\npopd\ndocker run --rm -v $CIFAR_DATA:$CIFAR_DATA -w $CIFAR_DATA $IMAGE_NAME /bin/bash -c \"source /cntk/activate-cntk; cp /cntk/Examples/Image/DataSets/CIFAR-10/* .; python convert_cifar10.py\"", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Additionally, we'll create an MPI helper script for executing the MPI job. This helper script does the following:\n1. Ensures that there are GPUs available to execute the task.\n2. Parses the `$AZ_BATCH_HOST_LIST` for all of the hosts participating in the MPI job and creates a `hostfile` from it\n3. Computes the total number of slots (processors)\n4. Sets the proper CNTK training directory, script and options\n5. Executes the MPI job via `mpirun`", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "%%writefile run_cifar10.sh\n#!/usr/bin/env bash\n\nset -e\nset -o pipefail\n\n# get number of GPUs on machine\nngpus=$(nvidia-smi -L | wc -l)\necho \"num gpus: $ngpus\"\n\nif [ $ngpus -eq 0 ]; then\n    echo \"No GPUs detected.\"\n    exit 1\nfi\n\n# get number of nodes\nIFS=',' read -ra HOSTS <<< \"$AZ_BATCH_HOST_LIST\"\nnodes=${#HOSTS[@]}\n\n# create hostfile\ntouch hostfile\n>| hostfile\nfor node in \"${HOSTS[@]}\"\ndo\n    echo $node slots=$ngpus max-slots=$ngpus >> hostfile\ndone\n\n# compute number of processors\nnp=$(($nodes * $ngpus))\n\n# print configuration\necho \"num nodes: $nodes\"\necho \"hosts: ${HOSTS[@]}\"\necho \"num mpi processes: $np\"\n\n# set cntk related vars\nmodeldir=/cntk/Examples/Image/Classification/ResNet/Python\ntrainscript=TrainResNet_CIFAR10_Distributed.py\n\n# set training options\ntrainopts=\"--datadir $AZ_BATCH_NODE_SHARED_DIR/cifar10_data --outputdir $AZ_BATCH_TASK_WORKING_DIR/output --network resnet20 -q 1 -a 0\"\n\n# execute mpi job\n/root/openmpi/bin/mpirun --allow-run-as-root --mca btl_tcp_if_exclude docker0 \\\n    -np $np --hostfile hostfile -x LD_LIBRARY_PATH --wdir $modeldir \\\n    /bin/bash -c \"source /cntk/activate-cntk; python -u $trainscript $trainopts $*\"", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Move the files into a directory to be uploaded.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "INPUT_CONTAINER = 'input-dist'\nUPLOAD_DIR = 'dist_upload'\n\n!rm -rf $UPLOAD_DIR\n!mkdir -p $UPLOAD_DIR\n!mv convert_cifar10.* run_cifar10.sh $UPLOAD_DIR\n!ls -alF $UPLOAD_DIR", "outputs": [], "metadata": {"collapsed": false}}, {"source": "We will create the config structure to directly reference these files to ingress into Azure Storage. This obviates the need to call `blobxfer` as it will be done for us during pool creation.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "STORAGE_ALIAS = 'mystorageaccount'\n\nconfig = {\n    \"batch_shipyard\": {\n        \"storage_account_settings\": STORAGE_ALIAS\n    },\n    \"global_resources\": {\n        \"docker_images\": [\n            IMAGE_NAME\n        ],\n        \"files\": [\n            {\n                \"source\": {\n                    \"path\": UPLOAD_DIR\n                },\n                \"destination\": {\n                    \"storage_account_settings\": STORAGE_ALIAS,\n                    \"data_transfer\": {\n                        \"container\": INPUT_CONTAINER\n                    }\n                }\n            }\n        ]\n    }\n}", "outputs": [], "metadata": {"collapsed": true}}, {"source": "Now we'll create the pool specification with a few modifications for this particular execution:\n- `inter_node_communication_enabled` will ensure nodes are allocated such that they can communicate with each other (e.g., send and receive network packets)\n- `input_data` specifies the scripts we created above to be downloaded into `$AZ_BATCH_NODE_SHARED_DIR/cifar10_data`\n- `transfer_files_on_pool_creation` will transfer the `files` specified in `global_resources` to be transferred during pool creation (i.e., `pool add`)\n- `resource_files` are the CNTK train and test data files\n- `additional_node_prep_commands` are commands to execute for node preparation of all compute nodes. Our additional node prep command is to execute the conversion script we created in an earlier step above\n\n**Note:** Most often it is better to scale up the execution first, prior to scale out. Due to our default core quota of just 20 cores, we are using 3 `STANDARD_NC6` nodes. In real production runs, we'd most likely scale up to multiple GPUs within a single node (parallel execution) such as `STANDARD_NC12` or `STANDARD_NC24` prior to scaling out to multiple NC nodes (parallel and distributed execution).", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "POOL_ID = 'gpupool-multi-instance'\n\npool = {\n    \"pool_specification\": {\n        \"id\": POOL_ID,\n        \"vm_size\": \"STANDARD_NC6\",\n        \"vm_count\": 3,\n        \"publisher\": \"Canonical\",\n        \"offer\": \"UbuntuServer\",\n        \"sku\": \"16.04-LTS\",\n        \"ssh\": {\n            \"username\": \"docker\"\n        },\n        \"inter_node_communication_enabled\": True,\n        \"reboot_on_start_task_failed\": False,\n        \"block_until_all_global_resources_loaded\": True,\n        \"input_data\": {\n            \"azure_storage\": [\n                {\n                    \"storage_account_settings\": STORAGE_ALIAS,\n                    \"container\": INPUT_CONTAINER,\n                    \"destination\": \"$AZ_BATCH_NODE_SHARED_DIR/cifar10_data\"\n                }\n            ]\n        },\n        \"transfer_files_on_pool_creation\": True,\n        \"resource_files\": create_resource_file_list(),\n        \"additional_node_prep_commands\": [\n            \"/bin/bash $AZ_BATCH_NODE_SHARED_DIR/cifar10_data/convert_cifar10.sh\"\n        ]\n    }\n}", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "!mkdir config\nwrite_json_to_file(config, os.path.join('config', 'config.json'))\nwrite_json_to_file(pool, os.path.join('config', 'pool.json'))\nprint(json.dumps(config, indent=4, sort_keys=True))\nprint(json.dumps(pool, indent=4, sort_keys=True))", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Create the pool, please be patient while the compute nodes are allocated.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "shipyard pool add -y", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Ensure that all compute nodes are `idle` and ready to accept tasks:", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "shipyard pool listnodes", "outputs": [], "metadata": {"collapsed": false}}, {"source": "<a id='section2'><\/a>", "cell_type": "markdown", "metadata": {}}, {"source": "## Configure MPI Job and Submit", "cell_type": "markdown", "metadata": {}}, {"source": "MPI jobs in Batch require execution as a multi-instance task. Essentially this allows multiple compute nodes to be used for a single task.\n\nA few things to note in this jobs configuration:\n- The `COMMAND` executes the `run_cifar10.sh` script that was uploaded earlier as part of the node preparation task.\n- `auto_complete` is being set to `True` which forces the job to move from `active` to `completed` state once all tasks complete. Note that once a job has moved to `completed` state, no new tasks can be added to it.\n- `multi_instance` property is populated which enables multiple nodes, e.g., `num_instances` to participate in the execution of this task. The `coordination_command` is the command that is run on all nodes prior to the `command`. Here, we are simply executing the Docker image to run the SSH server for the MPI daemon (e.g., orted, hydra, etc.) to initialize all of the nodes prior to running the application command.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "JOB_ID = 'cntk-mpi-job'\n\n# reduce the nubmer of epochs to 20 for purposes of this notebook\nCOMMAND = '$AZ_BATCH_NODE_SHARED_DIR/run_cifar10.sh -e 20'\n\njobs = {\n    \"job_specifications\": [\n        {\n            \"id\": JOB_ID,\n            \"auto_complete\": True,\n            \"tasks\": [\n                {\n                    \"image\": IMAGE_NAME,\n                    \"remove_container_after_exit\": True,\n                    \"command\": COMMAND,\n                    \"gpu\": True,\n                    \"multi_instance\": {\n                        \"num_instances\": \"pool_current_dedicated\",\n                        \"coordination_command\": \"/usr/sbin/sshd -D -p 23\"\n                    },\n                }\n            ],\n        }\n    ]\n}", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "write_json_to_file(jobs, os.path.join('config', 'jobs.json'))\nprint(json.dumps(jobs, indent=4, sort_keys=True))", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Submit the job and tail `stdout.txt`:", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "shipyard jobs add --tail stdout.txt", "outputs": [], "metadata": {"scrolled": false, "collapsed": false}}, {"source": "Using the command below we can check the status of our jobs. Once all jobs have an exit code we can continue. You can also view the **heatmap** of this pool on [Azure Portal](https://portal.azure.com) to monitor the progress of this job on the compute nodes under your Batch account.", "cell_type": "markdown", "metadata": {}}, {"source": "<a id='section3'><\/a>", "cell_type": "markdown", "metadata": {}}, {"source": "## Delete Multi-instance Job", "cell_type": "markdown", "metadata": {}}, {"source": "Deleting multi-instance jobs running as Docker containers requires a little more care. We will need to first ensure that the job has entered `completed` state. In the above `jobs` configuration, we set `auto_complete` to `True` enabling the Batch service to automatically complete the job when all tasks finish. This also allows automatic cleanup of the running Docker containers used for executing the MPI job.\n\nSpecial logic is required to cleanup MPI jobs since the `coordination_command` that runs actually detaches an SSH server. The job auto completion logic Batch Shipyard injects ensures that these containers are killed.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "shipyard jobs listtasks", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Once we are sure that the job is completed, then we issue the standard delete command:", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "shipyard jobs del -y --termtasks --wait", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## Next Steps\nYou can proceed to the [Notebook: Clean Up](05_Clean_Up.ipynb) if you are done for now, or proceed to one of the following additional Notebooks:\n* [Notebook: Automatic Model Selection](06_Advanced_Auto_Model_Selection.ipynb)\n* [Notebook: Tensorboard Visualization](07_Advanced_Tensorboard.ipynb) - note this requires running this notebook on your own machine\n* [Notebook: Keras with TensorFlow](09_Keras_Single_GPU_Training_With_Tensorflow.ipynb)", "cell_type": "markdown", "metadata": {}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Python 2", "name": "python2", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "2.7.11", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython2", "codemirror_mode": {"version": 2, "name": "ipython"}}, "anaconda-cloud": {}}}