{"nbformat_minor": 2, "cells": [{"source": "# IV - Parameter Sweep\nIn this notebook we will be running a simple parameter sweep on the model we have. We will then pull the results of our sweep and based on the results of our sweep pull the best performing model from blob.\n\n* [Setup](#section1)\n* [Configure job](#section2)\n* [Submit job](#section3)\n* [Check results](#section4)\n* [Download best model](#section5)\n* [Delete job](#section6)", "cell_type": "markdown", "metadata": {}}, {"source": "<a id='section1'><\/a>", "cell_type": "markdown", "metadata": {}}, {"source": "## Setup", "cell_type": "markdown", "metadata": {}}, {"source": "Create a simple alias for Batch Shipyard", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "%alias shipyard SHIPYARD_CONFIGDIR=config python $HOME/batch-shipyard/shipyard.py %l", "outputs": [], "metadata": {"collapsed": true}}, {"source": "Check that everything is working", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "shipyard", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Read in the account information we saved earlier", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "import json\n\ndef read_json(filename):\n    with open(filename, 'r') as infile:\n        return json.load(infile)\n    \naccount_info = read_json('account_information.json')\n\nstorage_account_key = account_info['storage_account_key']\nstorage_account_name = account_info['storage_account_name']", "outputs": [], "metadata": {"collapsed": true}}, {"source": "<a id='section2'><\/a>", "cell_type": "markdown", "metadata": {}}, {"source": "## Configure Job", "cell_type": "markdown", "metadata": {}}, {"source": "As in the previous job we ran on a single node we will be running the job on GPU enabled nodes. The difference here is that depending on the number of combinations we will be creating the same number of tasks. Each task will have a different set of parmeters that we will be passing to our model training script. This parameters effect the training of the model and in the end the performance of the model. The model and results of its evaluation are recorded and stored on the node. At the end of the task the results are pulled into the specified storage container..", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "from copy import copy\nimport os\nimport random\nfrom sklearn.grid_search import ParameterGrid\nfrom toolz import curry, pipe\n\ndef write_json_to_file(json_dict, filename):\n    \"\"\" Simple function to write JSON dictionaries to files\n    \"\"\"\n    with open(filename, 'w') as outfile:\n        json.dump(json_dict, outfile)\n\nCNTK_TRAIN_DATA_FILE = 'Train_cntk_text.txt'\nCNTK_TEST_DATA_FILE = 'Test_cntk_text.txt'\nURL_FMT = 'https://{}.blob.core.windows.net/{}/{}'\n\ndef select_random_data_storage_container():\n    \"\"\"Randomly select a storage account and container for CNTK train/test data.\n    This is specific for the workshop to help distribute attendee load. This\n    function will only work on Python2\"\"\"\n    ss = random.randint(0, 4)\n    cs = random.randint(0, 4)\n    sa = '{}{}bigai'.format(ss, chr(ord('z') - ss))\n    cont = '{}{}{}'.format(cs, chr(ord('i') - cs * 2), chr(ord('j') - cs * 2))\n    return sa, cont\n\ndef create_resource_file_list():\n    sa, cont = select_random_data_storage_container()\n    ret = [{\n        'file_path': CNTK_TRAIN_DATA_FILE,\n        'blob_source': URL_FMT.format(sa, cont, CNTK_TRAIN_DATA_FILE)\n    }]\n    sa, cont = select_random_data_storage_container()\n    ret.append({\n        'file_path': CNTK_TEST_DATA_FILE,\n        'blob_source': URL_FMT.format(sa, cont, CNTK_TEST_DATA_FILE)\n    })\n    return ret\n\ndef compose_command(num_convolution_layers, minibatch_size, max_epochs=30):\n    cmd_str = ' '.join((\"source /cntk/activate-cntk;\",\n                        \"python -u /code/ConvNet_CIFAR10.py\",\n                        \"--num_convolution_layers {num_convolution_layers}\",\n                        \"--minibatch_size {minibatch_size}\",\n                        \"--max_epochs {max_epochs}\")).format(num_convolution_layers=num_convolution_layers,\n                                                             minibatch_size=minibatch_size,\n                                                             max_epochs=max_epochs)\n    return 'bash -c \"{}\"'.format(cmd_str)\n\n@curry\ndef append_parameter(param_name, param_value, data_dict):\n    data_dict[param_name]=param_value\n    return data_dict\n\ndef task_generator(parameters):\n    for params in ParameterGrid(parameters):\n        yield pipe(copy(_TASK_TEMPLATE),\n                   append_parameter('command', compose_command(**params)),\n                   append_parameter('resource_files', create_resource_file_list()))", "outputs": [], "metadata": {"collapsed": true}}, {"source": "Generate the `jobs.json` configuration file", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "OUTPUT_STORAGE_ALIAS = \"mystorageaccount\"\n\nIMAGE_NAME = \"masalvar/cntkcifar\" # Custom CNTK image\n\n_TASK_TEMPLATE = {\n    \"image\": IMAGE_NAME,\n    \"remove_container_after_exit\": True,\n    \"gpu\": True,\n    \"output_data\": {\n        \"azure_storage\": [\n            {\n                \"storage_account_settings\": OUTPUT_STORAGE_ALIAS,\n                \"container\": \"output\",\n                \"source\": \"$AZ_BATCH_TASK_DIR/wd/Models\"\n            },\n        ]\n    },\n}", "outputs": [], "metadata": {"collapsed": false}}, {"source": "For the purposes of the workshop, we are constraining the parameter search space to just 3 final combinations (4 will be generated but we will remove the first one such that we only need to wait for \"one round\" of processing. In other words, since we have 3 total compute nodes in the pool and 3 tasks, we only have to wait for \"one round.\"", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "parameters = {\n    \"num_convolution_layers\": [2, 3],  # this could be expanded to [2, 3, 4] for example\n    \"minibatch_size\": [32, 64]         # this could be expanded to [32, 64, 128] for example\n}", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "JOB_ID = 'cntk-parametricsweep-job'\n\njobs = {\n    \"job_specifications\": [\n        {\n            \"id\": JOB_ID,\n            \"tasks\": list(task_generator(parameters))    \n        }\n    ]\n}\n\n# for purposes of expediency in the workshop, we'll remove one of the tasks to\n# make 3 total to match the number of compute nodes in our pool\ndel jobs['job_specifications'][0]['tasks'][0]\n\nprint('number of tasks for parametric sweep {}: {}'.format(JOB_ID, len(jobs['job_specifications'][0]['tasks'])))", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "write_json_to_file(jobs, os.path.join('config', 'jobs.json'))\nprint(json.dumps(jobs, indent=4, sort_keys=True))", "outputs": [], "metadata": {"collapsed": false}}, {"source": "<a id='section3'><\/a>", "cell_type": "markdown", "metadata": {}}, {"source": "## Submit job\nCheck that everything is ok with our pool before we submit our jobs", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "shipyard pool listnodes", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Now that we have confirmed everything is working we can execute our job using the command below. ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "shipyard jobs add", "outputs": [], "metadata": {"scrolled": false, "collapsed": false}}, {"source": "Using the command below we can check the status of our job. Only after all tasks have an exit code can we continue with the notebook. Please keep re-running the cell below periodically until you see that all tasks show completed state with an exit code. Continuing on with the notebook without all tasks in the job completing their training execution will result in failure in subsequent cells.\n\nYou can also view the **heatmap** of this pool on [Azure Portal](https://portal.azure.com) to monitor the progress of this job on the compute nodes under your Batch account.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "shipyard jobs listtasks --jobid $JOB_ID", "outputs": [], "metadata": {"collapsed": false}}, {"source": "<a id='section4'><\/a>", "cell_type": "markdown", "metadata": {}}, {"source": "# Check results\nThe results of our parameter search should now be saved to our output container.\n\n**Note:** You will encounter errors if you did not wait for all tasks to complete with an exit code in the previous cell.\n\nFirst let's alias `blobxfer` to aid in downloading our blobs. We will aggregate our results in the `MODELS_DIR`.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "%alias blobxfer python -m blobxfer\n\nMODELS_DIR = 'psmodels'", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "blobxfer $storage_account_name output $MODELS_DIR --remoteresource . --download --include \"*_$JOB_ID/model_results.json\" --storageaccountkey $storage_account_key", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Now we will combine all of the `model_results.json` files into one dictionary for analysis.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "def scandir(basedir):\n    for root, dirs, files in os.walk(basedir):\n        for f in files:\n            yield os.path.join(root, f) \n\nresults_dict = {}\nfor model in scandir(MODELS_DIR):\n    if not model.endswith('.json'):\n        continue\n    key = model.split(os.sep)[1]\n    results_dict[key] = read_json(model)\n    \nprint(json.dumps(results_dict, indent=4, sort_keys=True))", "outputs": [], "metadata": {"collapsed": false}}, {"source": "From the aggregated results dictionary, we select the one with the smallest error:", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "tuple_min_error = min(results_dict.iteritems(), key=lambda x: x[1]['test_metric'])\nconfiguration_with_min_error = tuple_min_error[0]\nprint('task with smallest error: {} ({})'.format(configuration_with_min_error, tuple_min_error[1]['test_metric']))", "outputs": [], "metadata": {"collapsed": false}}, {"source": "<a id='section5'><\/a>", "cell_type": "markdown", "metadata": {}}, {"source": "## Download best model\nNow we'll download the corresponding best performing model.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "MODEL_NAME = 'ConvNet_CIFAR10_model.dnn'\nBEST_MODEL_BLOB_NAME = '{}/{}'.format(configuration_with_min_error, MODEL_NAME)\nprint(BEST_MODEL_BLOB_NAME)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "blobxfer $storage_account_name output $MODELS_DIR --remoteresource $BEST_MODEL_BLOB_NAME --download --storageaccountkey $storage_account_key", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "!mv $MODELS_DIR/$configuration_with_min_error/$MODEL_NAME $MODELS_DIR\n!rm -rf $MODELS_DIR/*_$JOB_ID  # optionally remove all of the temporary result json directories/files\n!ls -alF $MODELS_DIR", "outputs": [], "metadata": {"collapsed": false}}, {"source": "The best model file (`ConvNet_CIFAR10_model.dnn`) is now ready for use.\n\n**Note:** We could have created a Batch task that did the model selection for us using task dependencies. The model selection task would be dependent upon all of the parametric sweep training tasks and would only run after those tasks complete successfully. The Batch task could then proceed with the logic above.\n\nPlease see the advanced notebook that shows how this is accomplished: [Automatic Model Selection from Parametric Sweep with Task Dependencies](06_Advanced_Auto_Model_Selection.ipynb)", "cell_type": "markdown", "metadata": {}}, {"source": "<a id='section6'><\/a>", "cell_type": "markdown", "metadata": {}}, {"source": "## Delete job", "cell_type": "markdown", "metadata": {}}, {"source": "To delete the job use the command below. Just be aware that this will get rid of all the files created by the job and tasks.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "shipyard jobs del -y --termtasks --wait", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## Next Steps\nYou can proceed to the [Notebook: Clean Up](05_Clean_up.ipynb) if you are done for now, or proceed to one of the following additional Notebooks:\n* [Notebook: Automatic Model Selection](06_Advanced_Auto_Model_Selection.ipynb)\n* [Notebook: Tensorboard Visualization](07_Advanced_Tensorboard.ipynb) - note this requires running this notebook on your own machine\n* [Notebook: Parallel and Distributed](08_Advanced_Parallel_and_Distributed.ipynb)\n* [Notebook: Keras with TensorFlow](09_Keras_Single_GPU_Training_With_Tensorflow.ipynb)", "cell_type": "markdown", "metadata": {}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Python 2", "name": "python2", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "2.7.11", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython2", "codemirror_mode": {"version": 2, "name": "ipython"}}, "anaconda-cloud": {}}}