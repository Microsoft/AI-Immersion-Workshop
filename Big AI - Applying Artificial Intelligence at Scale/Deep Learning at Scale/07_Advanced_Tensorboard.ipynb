{"nbformat_minor": 2, "cells": [{"source": "# VII - Tensorboard Visualization\nThis notebook will show how to create an SSH tunnel from the machine running the Notebook to the compute node of a task that is running or has run a task that has generated [Tensorboard](https://www.tensorflow.org/get_started/summaries_and_tensorboard) summary compatible output.\n\n**NOTE:** This notebook cannot be run on Azure notebooks due to restrictions. Please run this notebook locally. If you are running this notebook on Windows, please ensure you have `ssh.exe` in your `%PATH%`. You can download OpenSSH binaries for Windows [here](https://github.com/PowerShell/Win32-OpenSSH/releases).\n\n* [Setup](#section1)\n* [Configure job](#section2)\n* [Submit job](#section3)\n* [Delete job](#section4)", "cell_type": "markdown", "metadata": {}}, {"source": "<a id='section1'><\/a>", "cell_type": "markdown", "metadata": {}}, {"source": "## Setup", "cell_type": "markdown", "metadata": {}}, {"source": "Create a simple alias for Batch Shipyard", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "%alias shipyard SHIPYARD_CONFIGDIR=config python $HOME/batch-shipyard/shipyard.py %l", "outputs": [], "metadata": {"collapsed": true}}, {"source": "Check that everything is working", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "shipyard", "outputs": [], "metadata": {"collapsed": true}}, {"source": "<a id='section2'><\/a>\n## Configure job\nThe following will be similar to the [Single GPU Training](02_Single_GPU_Training.ipynb) notebook from earlier.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "import random\n\nCNTK_TRAIN_DATA_FILE = 'Train_cntk_text.txt'\nCNTK_TEST_DATA_FILE = 'Test_cntk_text.txt'\nURL_FMT = 'https://{}.blob.core.windows.net/{}/{}'\n\ndef select_random_data_storage_container():\n    \"\"\"Randomly select a storage account and container for CNTK train/test data.\n    This is specific for the workshop to help distribute attendee load. This\n    function will only work on Python2\"\"\"\n    ss = random.randint(0, 4)\n    cs = random.randint(0, 4)\n    sa = '{}{}bigai'.format(ss, chr(ord('z') - ss))\n    cont = '{}{}{}'.format(cs, chr(ord('i') - cs * 2), chr(ord('j') - cs * 2))\n    return sa, cont\n\ndef create_resource_file_list():\n    sa, cont = select_random_data_storage_container()\n    ret = [{\n        'file_path': CNTK_TRAIN_DATA_FILE,\n        'blob_source': URL_FMT.format(sa, cont, CNTK_TRAIN_DATA_FILE)\n    }]\n    sa, cont = select_random_data_storage_container()\n    ret.append({\n        'file_path': CNTK_TEST_DATA_FILE,\n        'blob_source': URL_FMT.format(sa, cont, CNTK_TEST_DATA_FILE)\n    })\n    return ret", "outputs": [], "metadata": {"collapsed": true}}, {"source": "For the `jobs` configuration, we will add `--logdir=tensorboard_logs` as a parameter to generate the Tensorboard summary log data during the run.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "TASK_ID = 'run_cifar10' # This should be changed per task\n\nJOB_ID = 'cntk-train-tensorboard-job'\n\nIMAGE_NAME = \"masalvar/cntkcifar\" # Custom CNTK image\n\nCOMMAND = 'bash -c \"source /cntk/activate-cntk; python -u /code/ConvNet_CIFAR10.py --logdir=tensorboard_logs\"'\n\nOUTPUT_STORAGE_ALIAS = \"mystorageaccount\"\n\njobs = {\n    \"job_specifications\": [\n        {\n            \"id\": JOB_ID,\n            \"tasks\": [\n                {\n                    \"id\": TASK_ID,\n                    \"image\": IMAGE_NAME,\n                    \"remove_container_after_exit\": True,\n                    \"command\": COMMAND,\n                    \"gpu\": True,\n                    \"resource_files\": create_resource_file_list(),\n                    \"output_data\": {\n                        \"azure_storage\": [\n                            {\n                                \"storage_account_settings\": OUTPUT_STORAGE_ALIAS,\n                                \"container\": \"output\",\n                                \"source\": \"$AZ_BATCH_TASK_WORKING_DIR/Models\"\n                            },\n                        ]\n                    },\n                }\n            ],\n        }\n    ]\n}", "outputs": [], "metadata": {"collapsed": true}}, {"source": "Write the jobs configuration to the `jobs.json` file:", "cell_type": "markdown", "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "import json\nimport os\n\ndef write_json_to_file(json_dict, filename):\n    \"\"\" Simple function to write JSON dictionaries to files\n    \"\"\"\n    with open(filename, 'w') as outfile:\n        json.dump(json_dict, outfile)\n\nwrite_json_to_file(jobs, os.path.join('config', 'jobs.json'))\nprint(json.dumps(jobs, indent=4, sort_keys=True))", "outputs": [], "metadata": {"collapsed": true}}, {"source": "<a id='section3'><\/a>", "cell_type": "markdown", "metadata": {}}, {"source": "## Submit job\nCheck that everything is ok with our pool before we submit our jobs\n", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "shipyard pool listnodes", "outputs": [], "metadata": {"collapsed": true}}, {"source": "Now that we have confirmed everything is working we can execute our job using the command below. Note that we'll not be using the `--tail` option so that the command completes and we can tunnel to Tensorboard concurrently as the task is executing.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "shipyard jobs add", "outputs": [], "metadata": {"scrolled": false, "collapsed": true}}, {"source": "Run the Batch Shipyard command to instantiate a Tensorboard instance and create an SSH tunnel. The following cell should not return immediately if it is working. Browse to the Tensorboard URL output by the command (which will not be output in the notebook since it is a blocking call), which is http://localhost:6006/\n\n**Notes:**\n1. The Tensorboard instance may take some time to start since this pool does not have the TensorFlow Docker image pre-loaded.\n2. You will need to manually interrupt the kernel once you are done with your Tensorboard visualization.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "shipyard misc tensorboard --jobid $JOB_ID --taskid $TASK_ID -y", "outputs": [], "metadata": {"scrolled": false, "collapsed": true}}, {"source": "<a id='section4'><\/a>", "cell_type": "markdown", "metadata": {}}, {"source": "## Delete job", "cell_type": "markdown", "metadata": {}}, {"source": "To delete the job use the command below. Just be aware that this will get rid of all the files created by the job and tasks.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "shipyard jobs del -y --termtasks --wait", "outputs": [], "metadata": {"scrolled": true, "collapsed": true}}, {"source": "## Next Steps\nYou can proceed to the [Notebook: Clean Up](05_Clean_Up.ipynb) if you are done for now, or proceed to one of the following additional Notebooks:\n* [Notebook: Automatic Model Selection](06_Advanced_Auto_Model_Selection.ipynb)\n* [Notebook: Parallel and Distributed](08_Advanced_Parallel_and_Distributed.ipynb)\n* [Notebook: Keras with TensorFlow](09_Keras_Single_GPU_Training_With_Tensorflow.ipynb)", "cell_type": "markdown", "metadata": {}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "3.5.1", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython3", "codemirror_mode": {"version": 3, "name": "ipython"}}}}